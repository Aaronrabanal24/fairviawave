name: Uptime Check

on:
  schedule:
    # Run every 15 minutes
    - cron: '*/15 * * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  health-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Unit tests
        run: npm test

      - name: Check Production Health
        run: |
          response=$(curl -s -w "%{http_code}" -o /tmp/response.json "${{ secrets.PRODUCTION_URL }}/api/health")

          if [ "$response" != "200" ]; then
            echo "‚ùå Health check failed with status: $response"
            cat /tmp/response.json
            exit 1
          fi

          if ! jq -e '.ok == true' /tmp/response.json > /dev/null; then
            echo "‚ùå Health check returned ok: false"
            cat /tmp/response.json
            exit 1
          fi

          echo "‚úÖ Health check passed"
          jq '.' /tmp/response.json

      - name: Check Metrics Summary Thresholds
        env:
          METRICS_URL: "${{ secrets.PRODUCTION_URL }}/api/metrics/summary"
        run: |
          python <<'PY'
import json
import os
import sys
import time
import urllib.error
import urllib.request

url = os.environ["METRICS_URL"]


def timed_fetch() -> tuple[int, bytes, float]:
    start = time.perf_counter()
    try:
        with urllib.request.urlopen(url, timeout=10) as resp:
            body = resp.read()
            status = resp.getcode()
    except urllib.error.URLError as exc:  # network issues / timeouts
        sys.exit(f"Failed to reach metrics endpoint: {exc}")
    elapsed_ms = (time.perf_counter() - start) * 1000
    return status, body, elapsed_ms


status, body, first_latency = timed_fetch()
print(f"Metrics status: {status}")

if status != 200:
    print(body.decode("utf-8", errors="ignore"))
    sys.exit(f"Metrics endpoint returned {status}")

try:
    data = json.loads(body.decode("utf-8"))
except json.JSONDecodeError as exc:
    sys.exit(f"Failed to parse metrics JSON: {exc}")

total_units = data.get("total_units") or 0
published_units = data.get("published_units") or 0
events_last_24h = data.get("events_last_24h") or 0

if total_units > 0:
    published_rate = published_units / total_units
    print(f"Published rate: {published_rate:.2%}")
    if published_rate < 0.30:
        sys.exit(f"Published rate below threshold: {published_rate:.2%}")
else:
    print("No units present; skipping publish rate threshold check.")

if published_units > 0 and events_last_24h == 0:
    sys.exit("Published units exist but no events recorded in the last 24h.")

latency_samples = [first_latency]

for _ in range(9):
    status, _, elapsed = timed_fetch()
    if status != 200:
        sys.exit(f"Metrics endpoint returned {status} during latency sampling")
    latency_samples.append(elapsed)

latency_samples.sort()
p95_index = max(int(round(len(latency_samples) * 0.95)) - 1, 0)
p95_latency = latency_samples[p95_index]

print("Latency samples (ms):", ", ".join(f"{sample:.1f}" for sample in latency_samples))
print(f"P95 latency: {p95_latency:.2f} ms")

if p95_latency > 500:
    sys.exit(f"P95 latency {p95_latency:.2f} ms exceeds 500 ms threshold")

print("‚úÖ Metrics thresholds passed")
PY

      - name: Public timeline smoke
        env:
          PRODUCTION_URL: "${{ secrets.PRODUCTION_URL }}"
          PUBLIC_UNIT_ID: "${{ secrets.PUBLIC_UNIT_ID }}"
          PUBLIC_UNIT_TOKEN: "${{ secrets.PUBLIC_UNIT_TOKEN }}"
        run: |
          curl -fsS "$PRODUCTION_URL/api/health" >/dev/null
          if [ -n "$PUBLIC_UNIT_ID" ] && [ -n "$PUBLIC_UNIT_TOKEN" ]; then
            curl -fsS "$PRODUCTION_URL/api/units/$PUBLIC_UNIT_ID/timeline/public?token=$PUBLIC_UNIT_TOKEN" >/dev/null
          else
            echo "Skipping authenticated public timeline smoke; secrets not configured."
          fi

      - name: Notify on Failure
        if: failure()
        run: |
          echo "üö® Production health check failed!"
          echo "Check the logs above for details"
          # Add notification logic here (Slack, Discord, email, etc.)

  quality-checks:
    needs: health-check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install project deps
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps

      - name: Install test tooling (no-save)
        run: |
          npm install --no-save @playwright/test @axe-core/playwright @lhci/cli

      - name: E2E (Playwright)
        env:
          E2E_BASE_URL: "${{ secrets.PRODUCTION_URL }}"
          E2E_UNIT_ID: "${{ secrets.E2E_UNIT_ID }}"
          E2E_UNIT_TOKEN: "${{ secrets.E2E_UNIT_TOKEN }}"
        run: npm run e2e

      - name: A11y (axe)
        env:
          E2E_BASE_URL: "${{ secrets.PRODUCTION_URL }}"
          E2E_UNIT_ID: "${{ secrets.E2E_UNIT_ID }}"
          E2E_UNIT_TOKEN: "${{ secrets.E2E_UNIT_TOKEN }}"
        run: npm run e2e -- tests/a11y-public.spec.ts

      - name: Lighthouse budgets
        env:
          LHCI_URL: "${{ secrets.PRODUCTION_URL }}/u/${{ secrets.E2E_UNIT_ID }}?token=${{ secrets.E2E_UNIT_TOKEN }}"
        run: npx lhci autorun --config=lighthouserc.json

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: playwright-report
          if-no-files-found: ignore

      - name: Upload Playwright traces
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-traces
          path: test-results
          if-no-files-found: ignore
